{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectECE4179_29889472.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9a791a6bd184313badaac8fa986c5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_240ff0e054044a0188eecdfb234cbcbe",
              "IPY_MODEL_67041a2ffafe45109b6981899d718792",
              "IPY_MODEL_bcebf0d30af34a0a9abefcdaa30bd7d5"
            ],
            "layout": "IPY_MODEL_67c09127a1164e0b851053a52884fdb2"
          }
        },
        "240ff0e054044a0188eecdfb234cbcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_234da2f298f04b0b9da2259e43ebd809",
            "placeholder": "​",
            "style": "IPY_MODEL_ebbe6a0df0cf41e9a67cc278d0711e3a",
            "value": "100%"
          }
        },
        "67041a2ffafe45109b6981899d718792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6f3aeacb59745478a480113b4d6d672",
            "max": 574673361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef10701a54f84a66b048ab02fda6c15d",
            "value": 574673361
          }
        },
        "bcebf0d30af34a0a9abefcdaa30bd7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_216b3bdac16e4241a96cbd2c7b066fc2",
            "placeholder": "​",
            "style": "IPY_MODEL_914cc771edc042baa434c5e9e588334b",
            "value": " 548M/548M [00:13&lt;00:00, 46.9MB/s]"
          }
        },
        "67c09127a1164e0b851053a52884fdb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234da2f298f04b0b9da2259e43ebd809": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebbe6a0df0cf41e9a67cc278d0711e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f3aeacb59745478a480113b4d6d672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef10701a54f84a66b048ab02fda6c15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "216b3bdac16e4241a96cbd2c7b066fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914cc771edc042baa434c5e9e588334b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "####Main Algorithm is copied from the following places \n",
        "################################################################*************************************************\n",
        "#L. A. Gatys, A. S. Ecker, and M. Bethge, \"Image style transfer using convolutional neural networks,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2414-2423. \n",
        "#A. K. Mallik. \"Neural Style Transfer Using PyTorch.\" https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa (accessed.\n",
        "#A. Ecker, A. Bethge, and L. Gatys, \"A neural algorithm of artistic style,\" arXiv preprint arXiv:1508.06576, 2015. \n",
        "\n",
        "\n",
        "##########################****************************************************************\n",
        "\n",
        "#### Combined 2 style images and obtained the results \n",
        "#### Experiments done on different convolutional layers"
      ],
      "metadata": {
        "id": "zwn4txZDsNCh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T01f_YIeXh6I"
      },
      "outputs": [],
      "source": [
        "\n",
        "#importing the library \n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loadung the model vgg19 that will serve as the base model\n",
        "base_model=models.vgg19(pretrained=True).features\n",
        "#Assigning the GPU to the variable device\n",
        "device=torch.device( \"cuda\" if (torch.cuda.is_available()) else 'cpu')"
      ],
      "metadata": {
        "id": "vwTHzqWmX3Qa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d9a791a6bd184313badaac8fa986c5ac",
            "240ff0e054044a0188eecdfb234cbcbe",
            "67041a2ffafe45109b6981899d718792",
            "bcebf0d30af34a0a9abefcdaa30bd7d5",
            "67c09127a1164e0b851053a52884fdb2",
            "234da2f298f04b0b9da2259e43ebd809",
            "ebbe6a0df0cf41e9a67cc278d0711e3a",
            "d6f3aeacb59745478a480113b4d6d672",
            "ef10701a54f84a66b048ab02fda6c15d",
            "216b3bdac16e4241a96cbd2c7b066fc2",
            "914cc771edc042baa434c5e9e588334b"
          ]
        },
        "outputId": "b0490dbc-b63d-4dc3-efa7-fc7955b58124"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9a791a6bd184313badaac8fa986c5ac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loader(path):\n",
        "    img = Image.open(path)\n",
        "    #resizing the image and convert image into a tensor \n",
        "    loader=transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
        "    #Adding an extra dimension at 0th index, for batch size \n",
        "    img = loader(img).unsqueeze(0)\n",
        "    return img.to(device, torch.float)\n",
        "\n",
        "#loading the content image and 2 style images \n",
        "content_image = image_loader('Content_Img.jpg')\n",
        "style_image1=image_loader('Style_Image.jpg')\n",
        "style_image2=image_loader('Style_Image_1.jpg')\n",
        "\n",
        "#cloning the content image \n",
        "gen_image = content_image.clone().requires_grad_(True)\n"
      ],
      "metadata": {
        "id": "52GUd_aDX4pc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a class that for the model\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG,self).__init__()\n",
        "        #'conv1_1',''conv2_1','conv3_1','conv4_1','conv5_1'\n",
        "        self.req_features= ['0','5','10','19','28'] #Original \n",
        "       #self.req_features= ['1','5','10','19','28'] #trial 1\n",
        "       #self.req_features= ['0','5','10','11','13'] #trial 2\n",
        "       #self.req_features= ['0','5','11','19','28'] #trial 3\n",
        "       #self.req_features= ['20','29','12','6','0'] #trial 4\n",
        "        #Since we need only the 5 layers in the model so we will be dropping all the rest layers from the features of the model\n",
        "        self.model=models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n",
        "    \n",
        "   \n",
        "    #x holds the input tensor(image) that will be feeded to each layer\n",
        "    def forward(self,x):\n",
        "        #initialize an array that wil hold the activations from the chosen layers\n",
        "        features=[]\n",
        "        #Iterate over all the layers of the mode\n",
        "        for layer_num,layer in enumerate(self.model):\n",
        "            #activation of the layer will stored in x\n",
        "            x=layer(x)\n",
        "            #appending the activation of the selected layers and return the feature array\n",
        "            if (str(layer_num) in self.req_features):\n",
        "                features.append(x)\n",
        "                \n",
        "        return features"
      ],
      "metadata": {
        "id": "wOh0B5B_v3J2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_content_loss(gen_feat,orig_feat):\n",
        "    #calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n",
        "    content_l=torch.mean((gen_feat-orig_feat)**2)\n",
        "    return content_l"
      ],
      "metadata": {
        "id": "nV7bRoW1wIs3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_style_loss(gen,style):\n",
        "    #Calculating the gram matrix for the style and the generated image\n",
        "    batch_size,channel,height,width=gen.shape\n",
        "\n",
        "    G=torch.mm(gen.view(channel,height*width),gen.view(channel,height*width).t())\n",
        "    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
        "        \n",
        "    #Calcultating the style loss of each layer by calculating the MSE between the gram matrix of the style image and the generated image and adding it to style loss\n",
        "    style_l=torch.mean((G-A)**2)\n",
        "    return style_l"
      ],
      "metadata": {
        "id": "2UG_MubuwKk-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(gen_features, orig_feautes, style_featues1, style_featues2):\n",
        "    style_loss1=content_loss=style_loss2=0\n",
        "    for gen,cont,style1,style2 in zip(gen_features,orig_feautes,style_featues1, style_featues2):\n",
        "        #extracting the dimensions from the generated image\n",
        "        content_loss+=calc_content_loss(gen,cont)\n",
        "        style_loss1+=calc_style_loss(gen,style1)\n",
        "        style_loss2+=calc_style_loss(gen,style2)\n",
        "    \n",
        "    #calculating the total loss of e th epoch\n",
        "    total_loss=alpha*content_loss + beta*style_loss1 + beta*style_loss2 \n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "y_I_XBdHwMnW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model to the GPU\n",
        "base_model=VGG().to(device).eval() \n",
        "\n",
        "#initialize the paramerters required for fitting the model\n",
        "epoch=7000\n",
        "lr=0.004\n",
        "alpha=8\n",
        "beta=70\n",
        "\n",
        "#using adam optimizer and it will update the generated image not the model parameter \n",
        "optimizer=optim.Adam([gen_image],lr=lr)"
      ],
      "metadata": {
        "id": "x3b1EAXewP-p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterating for 1000 times\n",
        "for e in range (epoch):\n",
        "    #extracting the features of generated, content and the original required for calculating the loss\n",
        "    gen_features=base_model(gen_image)\n",
        "    orig_feautes=base_model(content_image)\n",
        "    style_featues1=base_model(style_image1)\n",
        "    style_featues2=base_model(style_image2)\n",
        "    \n",
        "    #iterating over the activation of each layer and calculate the loss and add it to the content and the style loss\n",
        "    total_loss=calculate_loss(gen_features, orig_feautes, style_featues1,style_featues2 )\n",
        "    #optimize the pixel values of the generated image and backpropagate the loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    #print the image and save it after each 100 epoch\n",
        "    if(e/100):\n",
        "        print(total_loss)\n",
        "        \n",
        "        save_image(gen_image,\"gen.png\")  "
      ],
      "metadata": {
        "id": "_0lqcSVvwQXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}